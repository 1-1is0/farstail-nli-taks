{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:dml-qom/FarsTail.git data/FarsTail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "def rtl_print(outputs, font_size=\"15px\", n_to_br=False):\n",
    "    outputs = outputs if isinstance(outputs, list) else [outputs] \n",
    "    if n_to_br:\n",
    "        outputs = [output.replace('\\n', '') for output in outputs]\n",
    "        \n",
    "    outputs = [f'{output}' for output in outputs]\n",
    "    display.display(display.HTML(' '.join(outputs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/FarsTail/data/Train-word.csv', sep='\\t')\n",
    "val_df = pd.read_csv('./data/FarsTail/data/Val-word.csv', sep='\\t')\n",
    "test_df = pd.read_csv('./data/FarsTail/data/Test-word.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>اولین انتقال و نفوذ طبیعی فرهنگ و تمدن اسلامی ...</td>\n",
       "      <td>نخستین انتقال و نفوذ طبیعی فرهنگ و تمدن اسلامی...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اولین انتقال و نفوذ طبیعی فرهنگ و تمدن اسلامی ...</td>\n",
       "      <td>کانون های جغرافیایی مصر، اندلس و شام، نخستین ر...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اولین انتقال و نفوذ طبیعی فرهنگ و تمدن اسلامی ...</td>\n",
       "      <td>سیسیل بعد از اسپانیا بزرگ ترین کانونی بود که ه...</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ویژگی های هنر عصر اموی: ۱- تلفیقی بودن ۲- بازن...</td>\n",
       "      <td>نقاشی های تزئینی و تندیس های بی‌کیفیت، یکی از ...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ویژگی های هنر عصر اموی: ۱- تلفیقی بودن ۲- بازن...</td>\n",
       "      <td>با کیفیت بودن تندیس های دوره اموی، یکی از ویژگ...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise   \n",
       "0  اولین انتقال و نفوذ طبیعی فرهنگ و تمدن اسلامی ...  \\\n",
       "1  اولین انتقال و نفوذ طبیعی فرهنگ و تمدن اسلامی ...   \n",
       "2  اولین انتقال و نفوذ طبیعی فرهنگ و تمدن اسلامی ...   \n",
       "3  ویژگی های هنر عصر اموی: ۱- تلفیقی بودن ۲- بازن...   \n",
       "4  ویژگی های هنر عصر اموی: ۱- تلفیقی بودن ۲- بازن...   \n",
       "\n",
       "                                          hypothesis label  \n",
       "0  نخستین انتقال و نفوذ طبیعی فرهنگ و تمدن اسلامی...     e  \n",
       "1  کانون های جغرافیایی مصر، اندلس و شام، نخستین ر...     c  \n",
       "2  سیسیل بعد از اسپانیا بزرگ ترین کانونی بود که ه...     n  \n",
       "3  نقاشی های تزئینی و تندیس های بی‌کیفیت، یکی از ...     e  \n",
       "4  با کیفیت بودن تندیس های دوره اموی، یکی از ویژگ...     c  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label_id'] = label_encoder.fit_transform(train_df['label'])\n",
    "val_df['label_id'] = label_encoder.transform(val_df['label'])\n",
    "test_df['label_id'] = label_encoder.transform(test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = 50\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # x = torch.tensor(row['text'], dtype=torch.float32)\n",
    "        premise = row[\"premise\"]\n",
    "        hypothesis = row[\"hypothesis\"]\n",
    "        label = row[\"label\"]\n",
    "        label_id = row[\"label_id\"]\n",
    "        # text = f\"{self.tokenizer.cls_token} {premise} {self.tokenizer.sep_token} {hypothesis} {self.tokenizer.sep_token}\"\n",
    "        # Tokenize inputs\n",
    "        encoded_inputs = self.tokenizer.encode_plus(\n",
    "            premise,\n",
    "            hypothesis,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encoded_inputs[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoded_inputs[\"attention_mask\"].squeeze()\n",
    "        token_type_id = encoded_inputs[\"token_type_ids\"].squeeze()\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_id,\n",
    "            \"label\": label,\n",
    "            \"label_id\": torch.tensor(label_id, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention mask  \n",
    "https://huggingface.co/transformers/v3.2.0/glossary.html#attention-mask  \n",
    "token type id  \n",
    "https://huggingface.co/transformers/v3.2.0/glossary.html#token-type-ids  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config = AutoConfig.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "# model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "train_dataset = Dataset(train_df, tokenizer)\n",
    "val_dataset = Dataset(val_df, tokenizer)\n",
    "test_dataset = Dataset(test_df, tokenizer)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = cpu_count() - 2\n",
    "pin_memory = True if device == \"cuda\" else False\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import *\n",
    "# from utils.utils import build_batch\n",
    "\n",
    "\n",
    "class BertNLIModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        label_num=3,\n",
    "        reinit_num=0,\n",
    "        encoder_layer=12,\n",
    "        freeze_layers=True,\n",
    "    ):\n",
    "        super(BertNLIModel, self).__init__()\n",
    "        # self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(\n",
    "            \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "        )\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "        #     \"HooshvareLab/bert-base-parsbert-uncased\"\n",
    "        # )\n",
    "        self.bert = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "        self.bert.encoder.layer = self.bert.encoder.layer[:encoder_layer]\n",
    "\n",
    "        self.num_hidden_layers = self.config.num_hidden_layers\n",
    "        self.vdim = self.config.hidden_size\n",
    "        self.encoder_layer = encoder_layer\n",
    "\n",
    "        self.nli_head1 = nn.Linear(self.vdim, 250)\n",
    "        self.nli_head2 = nn.Linear(250, label_num)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        self.reinit(freeze=freeze_layers)\n",
    "\n",
    "    def reinit(self, freeze):\n",
    "        if freeze:\n",
    "            for _, pp in self.bert.named_parameters():\n",
    "                pp.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        cls_vecs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )[1]\n",
    "\n",
    "        logits = self.nli_head1(cls_vecs)\n",
    "        logits = self.nli_head2(logits)\n",
    "        probs = self.sm(logits)\n",
    "\n",
    "        torch.cuda.empty_cache()  # releases all unoccupied cached memory\n",
    "\n",
    "        return logits, probs\n",
    "\n",
    "    def _get_name(self):\n",
    "        return f\"{super()._get_name()}-{self.encoder_layer}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"BertNLIModel-Encoder-{self.encoder_layer}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"BertNLIModel-Encoder-{self.encoder_layer}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "\n",
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "token_type_ids = data['token_type_ids'].to(device)\n",
    "label_id = data['label_id'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "from easydict import EasyDict\n",
    "\n",
    "dataset = EasyDict(\n",
    "    {\n",
    "        \"train\": train_dataset,\n",
    "        \"val\": test_dataset,\n",
    "        \"test\": test_dataset,\n",
    "    }\n",
    ")\n",
    "data_loader = EasyDict(\n",
    "    {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": test_loader,\n",
    "        \"test\": test_loader,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/state-BertNLIModel-11-optimizer-AdamW-loss-CrossEntropyLoss.pth Not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BETTER NET STATE ###\n",
      "[61s] Epoch 1 loss : 1.05367951 acc: 44.12 val: 1.02515256 acc: 45.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 129/227 [01:31<01:32,  1.05it/s, epoch=2, phase=train, loss=1.0342]"
     ]
    }
   ],
   "source": [
    "model = BertNLIModel(label_num=3, encoder_layer=11)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=2e-4,eps=1e-6)\n",
    "\n",
    "start = train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=data_loader,\n",
    "    dataset=dataset,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
